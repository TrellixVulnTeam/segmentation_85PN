{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "MySemanticSegmentation - student.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4L5i2PzADAyA",
        "wWBYzpT9DAyA",
        "WNEKLq3TDAyA",
        "Vru8DzxNDAyA",
        "bVkM_1PwDAyA",
        "fUfIaDJXDAyA",
        "_X1CGJQNDAyA",
        "r5yb4IoFDAyA",
        "NV2XKxloDAyA",
        "novdLmuQDAyA",
        "j3-jN3z1DAyA",
        "Y1WlCPA_DAyA",
        "TBBEK629DAyA"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8470ccf28d5042cc8ee120679cde619b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_68b3713cff0d48d6b2d9abae14e00059",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3f346662749b4da5a1782396040c6c95",
              "IPY_MODEL_a9dc6c8dca1e48feab1079945761e492"
            ]
          }
        },
        "68b3713cff0d48d6b2d9abae14e00059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f346662749b4da5a1782396040c6c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2c6f753355ba484c91e253150634bdb2",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7644b0821f6544bc8bc75852b539e828"
          }
        },
        "a9dc6c8dca1e48feab1079945761e492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_49d96edab110415e81eec67d3822966b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 220MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d0ac230954ff42c59d3bacbdbdca6ff0"
          }
        },
        "2c6f753355ba484c91e253150634bdb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7644b0821f6544bc8bc75852b539e828": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49d96edab110415e81eec67d3822966b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d0ac230954ff42c59d3bacbdbdca6ff0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ojferro/segmentation/blob/main/MySemanticSegmentation_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkWRFvlpUVZ_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS2VgJr5DAx9"
      },
      "source": [
        "### CS 484/684 Computational Vision\n",
        "#### credits: many thanks for the design of this assignemnt go to <a href=\"https://tovacinni.github.io/\" target = \"_blank\">Towaki Takikawa</a> \n",
        "\n",
        "# Homework Assignment #5 - Supervised Deep Learning for Segmentation\n",
        "\n",
        "This assignment will test your understanding of applying deep learning by having you apply (fully supervised) deep learning to semantic segmentation, a well studied problem in computer vision. \n",
        "\n",
        "You can get most of the work done using only CPU, however, the use of GPU will be helpful in later parts. Programming and debugging everything upto and including problem 5c should be fine on CPU. You will notice the benefit of GPU mostly in later parts (d-h) of problem 5, but they are mainly implemenmted and test your code written and debugged earlier. If you do not have a GPU readily accesible to you, we recommend that you use Google Colaboratory to get access to a GPU. Once you are satisfied with your code upto and including 5(c), simply upload this Jupyter Notebook to Google Colaboratory to run the tests in later parts of Problem 5.\n",
        "\n",
        "Proficiency with PyTorch is required. Working through the PyTorch tutorials will make this assignment significantly easier. https://pytorch.org/tutorials/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCOdTnh4DAx-",
        "outputId": "ac824ebd-59d2-486f-c9de-86bf17a59eb4"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "drive.mount('/gdrive')\n",
        "!cp -r \"/gdrive/MyDrive/a5/images/\" .\n",
        "!cp -r \"/gdrive/MyDrive/a5/lib/\" .\n",
        "# !cp -r \"drive/My Drive/a5/datasets/\" .\n",
        "\n",
        "# It is best to start with USE_GPU = False (implying CPU). Switch USE_GPU to True only if you want to use GPU. However... \n",
        "# we strongly recommend to wait until you are absolutely sure your CPU-based code works (at least on single image dataset)\n",
        "USE_GPU = True"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD7G-ClYOHXD"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNru53mODSZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "329cc18e-e94f-4aaf-b953-8cec6df46856"
      },
      "source": [
        "# !wget https://data.deepai.org/PascalVOC2012.zip\n",
        "# ! ls drive/My\\ Drive/a5/datasets\n",
        "# ! mkdir -p drive/My\\ Drive/a5/datasets/PascalVOC2012\n",
        "# ! unzip drive/My\\ Drive/a5/datasets/datasets drive/My\\ Drive/a5/datasets/PascalVOC2012"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/a5/datasets/datasets\n",
            "caution: filename not matched:  drive/My Drive/a5/datasets/PascalVOC2012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfNq80RPGCUJ",
        "outputId": "8d6a99bd-2eef-4166-f903-bd34d101d9b3"
      },
      "source": [
        "# !cp ./datasets/PascalVOC2012.zip ./drive/My\\ Drive/a5/datasets\n",
        "# !mkdir -p  \"drive/My Drive/a5/datasets\"\n",
        "# ! ls 'drive/My Drive/a5/datasets' -la"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 7615707\n",
            "-rw------- 1 root root 3899239928 Dec  4 19:35 datasets\n",
            "drwx------ 2 root root       4096 Dec  4 19:46 PascalVOC2012\n",
            "-rw------- 1 root root 3899239928 Dec  4 19:39 PascalVOC2012.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhFVKCgpDAx-"
      },
      "source": [
        "# Python Libraries\n",
        "import random\n",
        "import math\n",
        "import numbers\n",
        "import platform\n",
        "import copy\n",
        "\n",
        "# Importing essential libraries for basic image manipulations.\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm as tqdm\n",
        "\n",
        "# We import some of the main PyTorch and TorchVision libraries used for HW4.\n",
        "# Detailed installation instructions are here: https://pytorch.org/get-started/locally/\n",
        "# That web site should help you to select the right 'conda install' command to be run in 'Anaconda Prompt'.\n",
        "# In particular, select the right version of CUDA. Note that prior to installing PyTorch, you should  \n",
        "# install the latest driver for your GPU and CUDA (9.2 or 10.1), assuming your GPU supports it. \n",
        "# For more information about pytorch refer to \n",
        "# https://pytorch.org/docs/stable/nn.functional.html\n",
        "# https://pytorch.org/docs/stable/data.html.\n",
        "# and https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as tF\n",
        "\n",
        "# We provide our own implementation of torchvision.datasets.voc (containing popular \"Pascal\" dataset)\n",
        "# that allows us to easily create single-image datasets\n",
        "from lib.voc import VOCSegmentation\n",
        "\n",
        "# Note class labels used in Pascal dataset:\n",
        "# 0:    background,\n",
        "# 1-20: aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, diningtable, dog, horse, motorbike,\n",
        "#       person, pottedplant, sheep, sofa, train, TV_monitor\n",
        "# 255: \"void\", which means class for pixel is undefined"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL72v4UWEC7t",
        "outputId": "ea445f34-11aa-4c92-a3b2-5f2884948685"
      },
      "source": [
        "! pip install chainercv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chainercv in /usr/local/lib/python3.6/dist-packages (0.13.1)\n",
            "Requirement already satisfied: chainer>=6.0 in /usr/local/lib/python3.6/dist-packages (from chainercv) (7.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from chainercv) (7.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer>=6.0->chainercv) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from chainer>=6.0->chainercv) (50.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from chainer>=6.0->chainercv) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from chainer>=6.0->chainercv) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from chainer>=6.0->chainercv) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer>=6.0->chainercv) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtr6pITCDAx-"
      },
      "source": [
        "# ChainerCV is a library similar to TorchVision, created and maintained by Preferred Networks. \n",
        "# Chainer, the base library, inspired and led to the creation of PyTorch! \n",
        "# Although Chainer and PyTorch are different, there are some nice functionalities in ChainerCV \n",
        "# that are useful, so we include it as an excersice on learning other libraries.\n",
        "# To install ChainerCV, normally it suffices to run \"pip install chainercv\" inside \"Anaconda Prompt\".\n",
        "# For more detailed installation instructions, see https://chainercv.readthedocs.io/en/stable/install.html\n",
        "# For other information about ChainerCV library, refer to https://chainercv.readthedocs.io/en/stable/ \n",
        "from chainercv.evaluations import eval_semantic_segmentation\n",
        "from chainercv.datasets import VOCSemanticSegmentationDataset"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wXIveVhDAx-"
      },
      "source": [
        "# This colorize_mask class takes in a numpy segmentation mask,\n",
        "#  and then converts it to a PIL Image for visualization.\n",
        "#  Since by default the numpy matrix contains integers from\n",
        "#  0,1,...,num_classes, we need to apply some color to this\n",
        "#  so we can visualize easier! Refer to:\n",
        "#  https://pillow.readthedocs.io/en/4.1.x/reference/Image.html#PIL.Image.Image.putpalette\n",
        "palette = [0, 0, 0, 128, 0, 0, 0, 128, 0, 128, 128, 0, 0, 0, 128, 128, 0, 128, 0, 128, 128,\n",
        "           128, 128, 128, 64, 0, 0, 192, 0, 0, 64, 128, 0, 192, 128, 0, 64, 0, 128, 192, 0, 128,\n",
        "           64, 128, 128, 192, 128, 128, 0, 64, 0, 128, 64, 0, 0, 192, 0, 128, 192, 0, 0, 64, 128]\n",
        "\n",
        "def colorize_mask(mask):\n",
        "    new_mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n",
        "    new_mask.putpalette(palette)\n",
        "\n",
        "    return new_mask"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYxdj3aujlt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb01f790-cbc2-4b5d-e165-0db4e9264865"
      },
      "source": [
        "# ! ls 'drive/My Drive/a5/dataset' ''\n",
        "# drive.mount('/gdrive')\n",
        "! ls '/gdrive/MyDrive/a5/dataset/JPEGImages' | grep '2007_0066'"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2007_006647.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "Rm7uII2rDAx-",
        "outputId": "b74537a8-cd48-415e-a6b9-7f6fc247166c"
      },
      "source": [
        "# Below we will use a sample image-target pair from VOC training dataset to test your joint transforms. \n",
        "# Running this block will automatically download the PASCAL VOC Dataset (3.7GB) to DATASET_PATH if \"download = True\".\n",
        "# The code below creates subdirectory \"datasets\" in the same location as the notebook file, but\n",
        "# you can modify DATASET_PATH to download the dataset to any custom directory. Download takes a few minutes.\n",
        "# On subsequent runs you may save time by setting \"download = False\" (the default value of this flag)\n",
        "\n",
        "DATASET_PATH = '/gdrive/MyDrive/a5/dataset'# 'datasets/PascalVOC2012/VOC2012'\n",
        "\n",
        "# Here, we obtain and visualize one sample (img, target) pair from VOC training dataset and one from validation dataset. \n",
        "# Note that operator [...] extracts the sample corresponding to the specified index. \n",
        "# Also, note the parameter download = True. Set this to False after you download to save time on later runs.\n",
        "sample1 = VOCSegmentation(DATASET_PATH, image_set='train', download = False)[0]\n",
        "sample2 = VOCSegmentation(DATASET_PATH, image_set='val')[20]\n",
        "\n",
        "# We demonstrate two different (equivalent) ways to access image and target inside the samples.\n",
        "img1, target1 = sample1\n",
        "img2 = sample2[0]\n",
        "target2 = sample2[1]\n",
        "\n",
        "fig = plt.figure(figsize=(14,10))\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "plt.title('sample1 - image')\n",
        "ax1.imshow(img1)\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "plt.title('sample1 - target')\n",
        "ax2.imshow(target1)\n",
        "ax3 = fig.add_subplot(2,2,3)\n",
        "plt.title('sample2 - image')\n",
        "ax3.imshow(img2)\n",
        "ax4 = fig.add_subplot(2,2,4)\n",
        "plt.title('sample2 - target')\n",
        "ax4.imshow(target2)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-b7e30ddf2c3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Note that operator [...] extracts the sample corresponding to the specified index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Also, note the parameter download = True. Set this to False after you download to save time on later runs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msample1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVOCSegmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0msample2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVOCSegmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/lib/voc.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mimage\u001b[0m \u001b[0msegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \"\"\"\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2809\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2810\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gdrive/MyDrive/a5/dataset/JPEGImages/2007_000032.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbzocMpsDAx_"
      },
      "source": [
        "# Problem 1 \n",
        "\n",
        "#### Implement a set of \"Joint Transform\" functions to perform data augmentation in your dataset. \n",
        "\n",
        "Neural networks are typically applied to transformed images. There are several important reasons for this: \n",
        "\n",
        "1. The image data should is in certain required format (i.e. consistent spacial resolution to batch). The images should also be normalized and converted to the \"tensor\" data format expected by pytorch libraries. \n",
        "\n",
        "2. Some transforms are used to perform randomized image domain transformations with the purpose of \"data augmentation\". \n",
        "\n",
        "In this exercise, you will implement a set of different transform functions to do both of these things. Note that unlike classification nets, training semantic segmentation networks requires that some of the transforms are applied to both image and the corresponding \"target\" (Ground Truth segmentation mask). We refer to such transforms and their compositions as \"Joint\". In general, your Transform classes should take as the input both the image and the target, and return a tuple of the transformed input image and target. Be sure to use critical thinking to determine if you can apply the same transform function to both the input and the output.\n",
        "\n",
        "For this problem you may use any of the `torchvision.transforms.functional` functions. For inspiration, refer to:\n",
        "#### https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "#### https://pytorch.org/docs/stable/torchvision/transforms.html#module-torchvision.transforms.functional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHZhc4FCDAx_"
      },
      "source": [
        "#### Example 1\n",
        "\n",
        "This class takes a img, target pair, and then transform the pair such that they are in `Torch.Tensor()` format.\n",
        "\n",
        "#### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOZZHVpnDAx_"
      },
      "source": [
        "class JointToTensor(object):\n",
        "    def __call__(self, img, target):\n",
        "        return tF.to_tensor(img), torch.from_numpy(np.array(target.convert('P'), dtype=np.int32)).long()\n",
        "\n",
        "class JointToPil(object):\n",
        "    def __call__(self, img, target):\n",
        "      return tF.to_pil_image(img), colorize_mask(target.numpy())"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p4yAal0Gy16"
      },
      "source": [
        ""
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "Uq44QVJVDAx_",
        "outputId": "05023056-7986-4d87-ecc6-133acbe2b357"
      },
      "source": [
        "# Check the transform by passing the image-target sample.\n",
        "\n",
        "JointToTensor()(*sample1)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-6d2806af55fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check the transform by passing the image-target sample.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mJointToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msample1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sample1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VFVO09sDAx_"
      },
      "source": [
        "#### Example 2: \n",
        "\n",
        "This class implements CenterCrop that takes an img, target pair, and then apply a crop about the center of the image such that the output resolution is $\\mbox{size} \\times \\mbox{size}$.\n",
        "\n",
        "#### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "JN4z5On5DAx_",
        "outputId": "64bf5606-e11c-4d18-a31b-a47e3fdebc37"
      },
      "source": [
        "class JointCenterCrop(object):\n",
        "    def __init__(self, size):\n",
        "        \"\"\"\n",
        "        params:\n",
        "            size (int) : size of the center crop\n",
        "        \"\"\"\n",
        "        self.size = size\n",
        "        \n",
        "    def __call__(self, img, target):\n",
        "        if not isinstance(img, torch.Tensor):\n",
        "            img, target = JointToTensor()(img, target)\n",
        "        return (tF.five_crop(img, self.size)[4], \n",
        "                tF.five_crop(target, self.size)[4])\n",
        "    \n",
        "img, target = JointToPil()(*JointCenterCrop(250)(*sample1))\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "ax1.imshow(img)\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "ax2.imshow(target)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-7dfea061cf43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                 tF.five_crop(target, self.size)[4])\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJointToPil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mJointCenterCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msample1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sample1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyuSpOI2DAx_"
      },
      "source": [
        "#### (a) Implement RandomFlip\n",
        "\n",
        "This class should take a img, target pair and then apply a horizontal flip across the vertical axis at random.\n",
        "\n",
        "#### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "YwKv6LbfDAx_",
        "outputId": "6faac177-1931-4974-c0aa-25b2322289dd"
      },
      "source": [
        "class JointRandomFlip(object):  \n",
        "    def __init__(self, p=0.5):\n",
        "        \"\"\"\n",
        "        params:\n",
        "            p (float) : probability [0,1] of horizontal flip\n",
        "        \"\"\"\n",
        "        self.p = p\n",
        "    def __call__(self, img, target):\n",
        "        if not isinstance(img, torch.Tensor):\n",
        "            img, target = JointToTensor()(img, target)\n",
        "        return (tF.hflip(img), tF.hflip(target)) if np.random.rand()<self.p else (img, target)\n",
        "    \n",
        "img, target = JointToPil()(*JointRandomFlip()(*sample1))\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "ax1.imshow(img)\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "ax2.imshow(target)\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-0d3fe20e39c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJointToPil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mJointRandomFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msample1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sample1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmiV2-WNDAyA"
      },
      "source": [
        "#### (b) Implement RandomResizeCrop\n",
        "\n",
        "This class should take a img, target pair and then resize the images by a random scale between $[\\mbox{minimum_scale}, \\mbox{maximum_scale}]$, crop a random location of the image by $\\min(\\mbox{size}, \\mbox{image_height}, \\mbox{image_width})$ (where the size is passed in as an integer in the constructor), and then resize to $\\mbox{size} \\times \\mbox{size}$ (again, the size passed in). The crop box should fit within the image.\n",
        "\n",
        "#### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiIE0APOoC3d"
      },
      "source": [
        ""
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "Pul-evIoDAyA",
        "outputId": "644ed3f9-6c62-48b2-cb44-723ecce43310"
      },
      "source": [
        "\n",
        "class JointRandomResizeCrop(object):  \n",
        "    def __init__(self, scale, size):\n",
        "        \"\"\"\n",
        "        params:\n",
        "            scale (tuple): (minimum_scale, maximum_scale)\n",
        "            size (float): size of output image (and potentially crop)\n",
        "        \"\"\"\n",
        "        self.scale = scale\n",
        "        self.size = size\n",
        "    def __call__(self, img, target):\n",
        "        if not isinstance(img, torch.Tensor):\n",
        "              img, target = JointToTensor()(img, target)\n",
        "        #concatenate img and target\n",
        "        t = torch.cat((img,target.reshape(1,*target.shape)))\n",
        "\n",
        "        # Resize to random size in scale (inclusive of both edge values)\n",
        "        s = np.random.uniform(*self.scale)\n",
        "        sz = int(t.shape[1]*s),  int(t.shape[2]*s)\n",
        "        t = tF.resize(t, sz, interpolation=PIL.Image.NEAREST)\n",
        "\n",
        "        crop = min(self.size,*t.shape[1:])\n",
        "        r, c = np.random.uniform(t.shape[1]-crop), np.random.uniform(t.shape[2]-crop)\n",
        "        # r, c = int(min(r, t.shape[1]-crop)), int(min(c, t.shape[2]-crop))\n",
        "        r, c = int(r), int(c)\n",
        "        t = tF.resized_crop(t, top=r, left=c, height=crop, width=crop, size=self.size, interpolation=PIL.Image.NEAREST)\n",
        "\n",
        "        return t[:3], t[-1]\n",
        "\n",
        "size=300\n",
        "scale=(0.8, 1.1) # min and max scaling factor \n",
        "img, target = JointToPil()(*JointRandomResizeCrop(scale=scale, size=size)(*sample1))\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "ax1.imshow(img)\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "ax2.imshow(target)\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-4761656a75e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# min and max scaling factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJointToPil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mJointRandomResizeCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msample1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sample1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcBRESoZLVab"
      },
      "source": [
        ""
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L5i2PzADAyA"
      },
      "source": [
        "#### (c) Implement Normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hH8T04qDAyA"
      },
      "source": [
        "This class should take a img, target pair and then normalize the images by subtracting the mean and dividing variance. \n",
        "\n",
        "#### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWhGW7vnDAyA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "679dd253-5e04-434a-ded9-110b038d2986"
      },
      "source": [
        "norm = ([0.485, 0.456, 0.406], \n",
        "        [0.229, 0.224, 0.225])\n",
        "\n",
        "class JointNormalize(object):  \n",
        "    def __call__(self, img, target):\n",
        "        if not isinstance(img, torch.Tensor):\n",
        "              img, target = JointToTensor()(img, target)\n",
        "        # img = tF.normalize(img, img.mean(), img.std())\n",
        "\n",
        "        #Normalize with ImageNet's mean and std. dev\n",
        "        img = tF.normalize(img, norm[0], norm[1])\n",
        "\n",
        "        return img, target\n",
        "        \n",
        "\n",
        "# img, target = JointToPil()(*JointNormalize()(*sample1))\n",
        "img, target = JointNormalize()(*sample1)\n",
        "# Convert to viewable values for plotting/debugging\n",
        "img1 = (img.permute(1,2,0) + torch.abs(img.min()))/(img.max()+torch.abs(img.min()))\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "ax1.imshow(img1)\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "ax2.imshow(colorize_mask(target.numpy()))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-98a4a7527e1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# img, target = JointToPil()(*JointNormalize()(*sample1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJointNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msample1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m# Convert to viewable values for plotting/debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sample1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "7QykwJ0OUaUG",
        "outputId": "88d883ca-e2ce-4bc9-b024-cc28fd68df36"
      },
      "source": [
        "# This shows that the image was normalized (using ImageNet mean and std_dev)\n",
        "img.mean(), img1.mean(), JointToTensor()(*sample1)[0].mean()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-ea54b8d37626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This shows that the image was normalized (using ImageNet mean and std_dev)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJointToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msample1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM3Uc-msDAyA"
      },
      "source": [
        "#### (d) Compose the transforms together: \n",
        "##### Use `JointCompose` (fully implemeted below) to compose the implemented transforms together in some random order. Verify the output makes sense and visualize it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzOSQYHzDAyA"
      },
      "source": [
        "# This class composes transofrmations from a given list of image transforms (expected in the argument). Such compositions \n",
        "# will be applied to the dataset during training. This cell is fully implemented.\n",
        "\n",
        "class JointCompose(object):\n",
        "    def __init__(self, transforms):\n",
        "        \"\"\"\n",
        "        params: \n",
        "           transforms (list) : list of transforms\n",
        "        \"\"\"\n",
        "        self.transforms = transforms\n",
        "\n",
        "    # We override the __call__ function such that this class can be\n",
        "    # called as a function i.e. JointCompose(transforms)(img, target)\n",
        "    # Such classes are known as \"functors\"\n",
        "    def __call__(self, img, target):\n",
        "        \"\"\"\n",
        "        params:\n",
        "            img (PIL.Image)    : input image\n",
        "            target (PIL.Image) : ground truth label\n",
        "        \"\"\"\n",
        "        assert img.size == target.size\n",
        "        for t in self.transforms:\n",
        "            img, target = t(img, target)\n",
        "        return img, target"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNHuXa57caFS"
      },
      "source": [
        ""
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TrawigdDAyA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "45c7af25-bf2a-4449-8867-4ad4294f1642"
      },
      "source": [
        "# Student Answer:\n",
        "\n",
        "size=300\n",
        "scale=(0.8, 1.1)\n",
        "tforms = [JointRandomFlip(), JointRandomResizeCrop(scale=scale, size=size), JointNormalize()]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###### PLOT RESULTS ######\n",
        "img, target = JointCompose(tforms)(*sample1)\n",
        "# Convert normalized image to viewable values [0,1] for plotting/debugging\n",
        "img1 = (img.permute(1,2,0) + torch.abs(img.min()))/(img.max()+torch.abs(img.min()))\n",
        "\n",
        "fig = plt.figure(figsize=(6,6))\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "ax1.imshow(img1)\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "ax2.imshow(colorize_mask(target.numpy()))\n",
        "\n",
        "\n",
        "img, target = JointCompose(tforms)(*sample2)\n",
        "# Convert normalized image to viewable values [0,1] for plotting/debugging\n",
        "img1 = (img.permute(1,2,0) + torch.abs(img.min()))/(img.max()+torch.abs(img.min()))\n",
        "\n",
        "ax3 = fig.add_subplot(2,2,3)\n",
        "ax3.imshow(img1)\n",
        "ax4 = fig.add_subplot(2,2,4)\n",
        "ax4.imshow(colorize_mask(target.numpy()))\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-362246b7bc9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m###### PLOT RESULTS ######\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJointCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msample1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# Convert normalized image to viewable values [0,1] for plotting/debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sample1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoas-QDQa5Hj"
      },
      "source": [
        "## Output makes sense\n",
        "\n",
        "The resulting images are randomly flipped, cropped at random points, and normalized. Note that the normalized images cannot be plotted (as they contain negative values), so the above images are converted to the [0,1] range to be plotted. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xCtbKyFDAyA"
      },
      "source": [
        "#### (e) Compose the transforms together: use `JointCompose` to compose the implemented transforms for:\n",
        "#### 1. A sanity dataset that will contain 1 single image. Your objective is to overfit on this 1 image, so choose your transforms and parameters accordingly.\n",
        "#### 2. A training dataset that will contain the training images. The goal here is to generalize to the validation set, which is unseen.\n",
        "#### 3. A validation dataset that will contain the validation images. The goal here is to measure the 'true' performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrkf35gCDAyA"
      },
      "source": [
        "# Student Answer:\n",
        "\n",
        "\n",
        "size=250 #Get output to be 300\n",
        "scale=(0.6, 1.4) # +- 40% scaling\n",
        "\n",
        "#No data augmentation required if goal is to overfit. Just apply normalization\n",
        "sanity_joint_transform = [JointNormalize()]\n",
        "\n",
        "train_joint_transform = [JointRandomFlip(), JointRandomResizeCrop(scale=scale, size=size), JointNormalize()]\n",
        "\n",
        "val_joint_transform = [JointRandomFlip(), JointRandomResizeCrop(scale=scale, size=size), JointNormalize()]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjqiM4HbDAyA"
      },
      "source": [
        "This code below will then apply `train_joint_transform` to the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK_RZGJaDAyA"
      },
      "source": [
        "# Apply the Joint-Compose transformations above to create three datasets and the corresponding Data-Loaders.\n",
        "# This cell is fully implemented.\n",
        "\n",
        "# This single image data(sub)set can help to better understand and to debug the network training process. \n",
        "# Optional integer parameter 'sanity_check' specifies the index of the image-target pair and creates a single image dataset.\n",
        "# Note that we use the same image (index=200) as used for sample1.\n",
        "sanity_data = VOCSegmentation(\n",
        "    DATASET_PATH, \n",
        "    image_set = 'train',\n",
        "    transforms = sanity_joint_transform,\n",
        "    sanity_check = 200\n",
        ")\n",
        "\n",
        "# This is a standard VOC data(sub)set used for training semantic segmentation networks\n",
        "train_data = VOCSegmentation(\n",
        "    DATASET_PATH, \n",
        "    image_set = 'train', \n",
        "    transforms = train_joint_transform\n",
        ")\n",
        "\n",
        "# This is a standard VOC data(sub)set used for validating semantic segmentation networks\n",
        "val_data = VOCSegmentation(\n",
        "    DATASET_PATH, \n",
        "    image_set='val',\n",
        "    transforms = val_joint_transform\n",
        ")\n",
        "\n",
        "# Increase TRAIN_BATCH_SIZE if you are using GPU to speed up training. \n",
        "# When batch size changes, the learning rate may also need to be adjusted. \n",
        "# Note that batch size maybe limited by your GPU memory, so adjust if you get \"run out of GPU memory\" error.\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "\n",
        "# If you are NOT using Windows, set NUM_WORKERS to anything you want, e.g. NUM_WORKERS = 4,\n",
        "# but Windows has issues with multi-process dataloaders, so NUM_WORKERS must be 0 for Windows.\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "sanity_loader = DataLoader(sanity_data, batch_size=1, num_workers=NUM_WORKERS, shuffle=False)\n",
        "train_loader = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=1, num_workers=NUM_WORKERS, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE5doNsADAyA"
      },
      "source": [
        "# Problem 2\n",
        "\n",
        "#### (a) Implement encoder/decoder segmentation CNN using PyTorch.\n",
        "\n",
        "You must follow the general network architecture specified in the image below. Note that since convolutional layers are the main building blocks in common network architectures for image analysis, the corresponding blocks are typically unlabeled in the network diagrams. The network should have 5 (pre-trained) convolutional layers (residual blocks) from \"resnet\" in the encoder part, two upsampling layers, and one skip connection. For the layer before the final upsampling layer, lightly experiment with some combination of Conv, ReLU, BatchNorm, and/or other layers to see how it affects performance.   \n",
        "<img src=\"images/deeplabv2_overview.png\"> \n",
        "You should choose specific parameters for all layers, but the overall structure should be restricted to what is shown in the illustration above. For inspiration, you can refer to papers in the citation section of the following link to DeepLab (e.g. specific parameters for each layer): http://liangchiehchen.com/projects/DeepLab.html. The first two papers in the citation section are particularly relevant. \n",
        "\n",
        "In your implementation, you can use a base model of choice (you can use `torchvision.models` as a starting point), but we suggest that you learn the properties of each base model and choose one according to the computational resources available to you.\n",
        "\n",
        "##### Note: do not apply any post-processing (such as DenseCRF) to the output of your net.\n",
        "\n",
        "#### Solution:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8470ccf28d5042cc8ee120679cde619b",
            "68b3713cff0d48d6b2d9abae14e00059",
            "3f346662749b4da5a1782396040c6c95",
            "a9dc6c8dca1e48feab1079945761e492",
            "2c6f753355ba484c91e253150634bdb2",
            "7644b0821f6544bc8bc75852b539e828",
            "49d96edab110415e81eec67d3822966b",
            "d0ac230954ff42c59d3bacbdbdca6ff0"
          ]
        },
        "id": "1zv7oVrpidTW",
        "outputId": "47d92670-f53b-4e32-db32-a50fdeb92639"
      },
      "source": [
        "res18 = models.resnet18(pretrained=True)\n",
        "res18_conv = nn.Sequential(*list(res18.children())[4:9])\n",
        "res18_conv"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8470ccf28d5042cc8ee120679cde619b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (4): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhX60lysDAyA"
      },
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self, num_classes, criterion=None):\n",
        "        super(MyNet, self).__init__()\n",
        "        \n",
        "    \n",
        "    def forward(self, inp, gts=None):\n",
        "\n",
        "        # Implement me\n",
        "        \n",
        "        if self.training:\n",
        "            # Return the loss if in training mode\n",
        "            return self.criterion(lfinal, gts)              \n",
        "        else:\n",
        "            # Return the actual prediction otherwise\n",
        "            return lfinal\n",
        "\n",
        "\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmQbdsOHDAyA"
      },
      "source": [
        "#### (b) Create UNTRAINED_NET and run on a sample image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg4cveQlDAyA"
      },
      "source": [
        "untrained_net = MyNet(21).eval()\n",
        "sample_img, sample_target = JointNormalize(*norm)(*JointToTensor()(*sample1))\n",
        "untrained_output = untrained_net.forward(sample_img[None])\n",
        "\n",
        "fig = plt.figure(figsize=(14,10))\n",
        "ax = fig.add_subplot(1,3,1)\n",
        "plt.title('image sample')\n",
        "ax.imshow(sample1[0])\n",
        "ax = fig.add_subplot(1,3,2)\n",
        "plt.title('ground truth (target)')\n",
        "ax.imshow(sample1[1])\n",
        "ax = fig.add_subplot(1,3,3)\n",
        "plt.title('UNTRAINED_NET output/prediction')\n",
        "ax.imshow(colorize_mask(torch.argmax(untrained_output, dim=1).numpy()[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RINsLP73DAyA"
      },
      "source": [
        "# Problem 3 \n",
        "\n",
        "#### (a) Implement the loss function (Cross Entropy Loss). Do not use already implemented versions of this loss function.\n",
        "\n",
        "Feel free to use functions like `F.log_softmax` and `F.nll_loss` (if you want to, or you can just implement the math).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fUhfFknDAyA"
      },
      "source": [
        "# Student Answer:\n",
        "\n",
        "class MyCrossEntropyLoss(object):\n",
        "    def __call__(self, output, target):\n",
        "\n",
        "        return output, target\n",
        "\n",
        "def my_loss(output, target):\n",
        "    loss = torch.mean((output - target)**2)\n",
        "    return loss\n",
        "\n",
        "class JointCompose(object):\n",
        "    def __call__(self, img, target):\n",
        "        \"\"\"\n",
        "        params:\n",
        "            img (PIL.Image)    : input image\n",
        "            target (PIL.Image) : ground truth label\n",
        "        \"\"\"\n",
        "        assert img.size == target.size\n",
        "        for t in self.transforms:\n",
        "            img, target = t(img, target)\n",
        "        return img, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWBYzpT9DAyA"
      },
      "source": [
        "#### (b) Compare against the existing CrossEntropyLoss function on your sample output from your neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B56OZA-zDAyA"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "print(criterion(untrained_output, sample_target[None]))\n",
        "\n",
        "my_criterion = MyCrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "print(my_criterion(untrained_output, sample_target[None]))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWThBI8qDAyA"
      },
      "source": [
        "# Problem 4\n",
        "\n",
        "#### (a) Use standard function `eval_semantic_segmentation` (already imported from chainerCV) to compute \"mean intersection over union\" for the output of UNTRAINED_NET on sample1 (`untrained_output`) using the target for sample1. Read documentations for function  `eval_semantic_segmentation` to properly set its input parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOTwSxaiDAyA"
      },
      "source": [
        "# Write code to propely compute 'pred' and 'gts' as arguments for function 'eval_semantic_segemntation'\n",
        "\n",
        "# pred =\n",
        "# gts =\n",
        "\n",
        "conf = eval_semantic_segmentation(pred, gts)\n",
        "\n",
        "print(\"mIoU for the sample image / ground truth pair: {}\".format(conf['miou']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNEKLq3TDAyA"
      },
      "source": [
        "#### (b) Write the validation loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ026uedDAyA"
      },
      "source": [
        "def validate(val_loader, net):\n",
        "    \n",
        "    iou_arr = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "        \n",
        "            inputs, masks = data\n",
        "\n",
        "            if USE_GPU:\n",
        "                # Write me\n",
        "            else: \n",
        "                # Write me\n",
        "            \n",
        "            # Write me\n",
        "\n",
        "            # Hint: make sure the range of values of the ground truth is what you expect\n",
        "\n",
        "            conf = eval_semantic_segmentation(preds, gts)\n",
        "\n",
        "            iou_arr.append(conf['miou'])\n",
        "    \n",
        "    return val_loss, (sum(iou_arr) / len(iou_arr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vru8DzxNDAyA"
      },
      "source": [
        "#### (c) Run the validation loop for UNTRAINED_NET against the sanity validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwU3EYgjDAyA"
      },
      "source": [
        "%%time\n",
        "print(\"mIoU over the sanity dataset:{}\".format(validate(sanity_loader, untrained_net)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAA3WLMCDAyA"
      },
      "source": [
        "# Problem 5\n",
        "\n",
        "#### (a) Define an optimizer to train the given loss function.\n",
        "\n",
        "Feel free to choose your optimizer of choice from https://pytorch.org/docs/stable/optim.html.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCUkRqEHDAyA"
      },
      "source": [
        "def get_optimizer(net):\n",
        "    # Write me\n",
        "    return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVkM_1PwDAyA"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#### (b) Write the training loop to train the network. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kmZf86YDAyA"
      },
      "source": [
        "def train(train_loader, net, optimizer, loss_graph):\n",
        "    \n",
        "    for i, data in enumerate(train_loader):\n",
        "        \n",
        "        inputs, masks = data\n",
        "\n",
        "        if USE_GPU:\n",
        "            # Write me\n",
        "        \n",
        "        # Write me\n",
        "        \n",
        "        # loss_graph.append() Populate this list to graph the loss\n",
        "        \n",
        "    return main_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUfIaDJXDAyA"
      },
      "source": [
        "#### (c) Create OVERFIT_NET and train it on the single image dataset.\n",
        "##### Single image training is helpful for debugging and hyper-parameter tuning (e.g. learning rate, etc.) as it is fast even on a single CPU. In particular, you can work with a single image until your loss function is consistently decreasing during training loop and the network starts producing a reasonable output for this training image. Training on a single image also teaches about overfitting, particualrly when comparing it with more thorough forms of network training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSYU7b8VDAyA"
      },
      "source": [
        "%%time\n",
        "%matplotlib notebook\n",
        "\n",
        "# The whole training on a single image (20-40 epochs) should take only a minute or two on a CPU (and a few seconds on GPU). \n",
        "# Below we create a (deep) copy of untrained_net and train it on a single training image (leading to gross overfitting).\n",
        "# Later, we will create a separate (deep) copy of untrained_net to be trained on full training dataset.\n",
        "# NOTE: Normally, one can create a new net via declaration new_net = MyNet(21). But, randomization of weights when new nets \n",
        "# are declared that way creates *different* untrained nets. This notebook compares different versions of network training. \n",
        "# For this comparison to be direct and fair, it is better to train (deep) copies of the exact same untrained_net. \n",
        "overfit_net = copy.deepcopy(untrained_net)\n",
        "\n",
        "# set loss function for the net\n",
        "overfit_net.criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "# You can change the number of EPOCHS\n",
        "EPOCH = 40\n",
        "\n",
        "# switch to train mode (original untrained_net was set to eval mode)\n",
        "overfit_net.train()\n",
        "\n",
        "optimizer = get_optimizer(overfit_net)\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "\n",
        "loss_graph = []\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "plt.subplots_adjust(bottom=0.2,right=0.85,top=0.95)\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "\n",
        "for e in range(EPOCH):\n",
        "    loss = train(sanity_loader, overfit_net, optimizer, loss_graph)\n",
        "    ax.clear()\n",
        "    ax.set_xlabel('iterations')\n",
        "    ax.set_ylabel('loss value')\n",
        "    ax.set_title('Training loss curve for OVERFIT_NET')\n",
        "    ax.plot(loss_graph, label='training loss')\n",
        "    ax.legend(loc='upper right')\n",
        "    fig.canvas.draw()\n",
        "    print(\"Epoch: {} Loss: {}\".format(e, loss))\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X1CGJQNDAyA"
      },
      "source": [
        "#### Qualitative and quantitative evaluation of predictions (untrained vs overfit nets) - fully implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-QuVp-UDAyA"
      },
      "source": [
        "# switch back to evaluation mode\n",
        "overfit_net.eval()\n",
        "\n",
        "sample_img, sample_target = JointNormalize(*norm)(*JointToTensor()(*sample1))\n",
        "if USE_GPU:\n",
        "    sample_img = sample_img.cuda()\n",
        "sample_output_O = overfit_net.forward(sample_img[None])\n",
        "sample_output_U = untrained_net.forward(sample_img[None])\n",
        "\n",
        "# computing mIOU (quantitative measure of accuracy for network predictions)\n",
        "if USE_GPU:\n",
        "    pred_O = torch.argmax(sample_output_O, dim=1).cpu().numpy()[0]\n",
        "    pred_U = torch.argmax(sample_output_U, dim=1).cpu().numpy()[0]\n",
        "else:\n",
        "    pred_O = torch.argmax(sample_output_O, dim=1).numpy()[0]\n",
        "    pred_U = torch.argmax(sample_output_U, dim=1).numpy()[0]\n",
        "\n",
        "gts = torch.from_numpy(np.array(sample1[1].convert('P'), dtype=np.int32)).long().numpy()\n",
        "gts[gts == 255] = -1\n",
        "conf_O = eval_semantic_segmentation(pred_O[None], gts[None])\n",
        "conf_U = eval_semantic_segmentation(pred_U[None], gts[None])\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(14,10))\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "plt.title('image sample')\n",
        "ax1.imshow(sample1[0])\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "plt.title('ground truth (target)')\n",
        "ax2.imshow(sample1[1])\n",
        "ax3 = fig.add_subplot(2,2,3)\n",
        "plt.title('UNTRAINED_NET prediction')\n",
        "ax3.text(10, 25, 'mIoU = {:_>8.6f}'.format(conf_U['miou']), fontsize=20, color='white')\n",
        "ax3.imshow(colorize_mask(torch.argmax(sample_output_U, dim=1).cpu().numpy()[0]))\n",
        "ax4 = fig.add_subplot(2,2,4)\n",
        "plt.title('OVERFIT_NET prediction (for its training image)')\n",
        "ax4.text(10, 25, 'mIoU = {:_>8.6f}'.format(conf_O['miou']), fontsize=20, color='white')\n",
        "ax4.imshow(colorize_mask(torch.argmax(sample_output_O, dim=1).cpu().numpy()[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g93MG17DAyA"
      },
      "source": [
        "sample_img, sample_target = JointNormalize(*norm)(*JointToTensor()(*sample2))\n",
        "if USE_GPU:\n",
        "    sample_img = sample_img.cuda()\n",
        "sample_output_O = overfit_net.forward(sample_img[None])\n",
        "sample_output_U = untrained_net.forward(sample_img[None])\n",
        "\n",
        "# computing mIOU (quantitative measure of accuracy for network predictions)\n",
        "if USE_GPU:\n",
        "    pred_O = torch.argmax(sample_output_O, dim=1).cpu().numpy()[0]\n",
        "    pred_U = torch.argmax(sample_output_U, dim=1).cpu().numpy()[0]\n",
        "else:\n",
        "    pred_O = torch.argmax(sample_output_O, dim=1).numpy()[0]\n",
        "    pred_U = torch.argmax(sample_output_U, dim=1).numpy()[0]\n",
        "\n",
        "gts = torch.from_numpy(np.array(sample2[1].convert('P'), dtype=np.int32)).long().numpy()\n",
        "gts[gts == 255] = -1\n",
        "conf_O = eval_semantic_segmentation(pred_O[None], gts[None])\n",
        "conf_U = eval_semantic_segmentation(pred_U[None], gts[None])\n",
        "\n",
        "fig = plt.figure(figsize=(14,10))\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "plt.title('image sample')\n",
        "ax1.imshow(sample2[0])\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "plt.title('ground truth (target)')\n",
        "ax2.imshow(sample2[1])\n",
        "ax3 = fig.add_subplot(2,2,3)\n",
        "plt.title('UNTRAINED_NET prediction')\n",
        "ax3.text(10, 25, 'mIoU = {:_>8.6f}'.format(conf_U['miou']), fontsize=20, color='white')\n",
        "ax3.imshow(colorize_mask(torch.argmax(sample_output_U, dim=1).cpu().numpy()[0]))\n",
        "ax4 = fig.add_subplot(2,2,4)\n",
        "plt.title('OVERFIT_NET prediction (for image it has not seen)')\n",
        "ax4.text(10, 25, 'mIoU = {:_>8.6f}'.format(conf_O['miou']), fontsize=20, color='white')\n",
        "ax4.imshow(colorize_mask(torch.argmax(sample_output_O, dim=1).cpu().numpy()[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5yb4IoFDAyA"
      },
      "source": [
        "#### Run the validation loop for OVERFIT_NET against the sanity dataset (an image it was trained on) - fully implemented"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jrGqJEd6DAyA"
      },
      "source": [
        "%%time\n",
        "print(\"mIoU for OVERFIT_NET over its training image:{}\".format(validate(sanity_loader, overfit_net)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "743epnZuDAyA"
      },
      "source": [
        "### WARNING: For the remaining part of the assignment (below) it is advisable to switch to GPU mode as running each validation and training loop on the whole training set takes over an hour on CPU (there are several such loops below). Note that GPU mode is helpful only if you have a sufficiently good NVIDIA gpu (not older than 2-3 years) and cuda installed on your computer. If you do not have a sufficiently good graphics card available, you can still finish the remaining part in CPU mode (takes a few hours), as the cells below are mostly implemented and test your code written and debugged in the earlier parts above. You can also switch to Google Colaboratory to run the remaining parts below.\n",
        "\n",
        "### You can use validation-data experiments below to tune your hyper-parameters. Normally, validation data is used exactly for this purpose. For actual competitions, testing data is not public and you can not tune hyper-parameters on in. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV2XKxloDAyA"
      },
      "source": [
        "#### (d) Evaluate UNTRAINED_NET and OVERFIT_NET on validation dataset.\n",
        "##### Run the validation loop for UNTRAINED_NET against the validation dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOBo5rF2DAyA"
      },
      "source": [
        "%%time\n",
        "# This will be slow on CPU (around 1 hour or more). On GPU it should take only a few minutes (depending on your GPU).\n",
        "print(\"mIoU for UNTRAINED_NET over the entire dataset:{}\".format(validate(val_loader, untrained_net)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5ZAjAmEDAyA"
      },
      "source": [
        "##### Run the validation loop for OVERFIT_NET against the validation dataset (it has not seen): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbxgqYPsDAyA"
      },
      "source": [
        "%%time\n",
        "# This will be slow on CPU (around 1 hour or more). On GPU it should take only a few minutes (depending on your GPU).\n",
        "print(\"mIoU for OVERFIT_NET over the validation dataset:{}\".format(validate(val_loader, overfit_net)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "novdLmuQDAyA"
      },
      "source": [
        "#### (e) Explain in a few sentences the quantitative results observed in (c) and (d):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlDq5fdXDAyA"
      },
      "source": [
        "Student answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3-jN3z1DAyA"
      },
      "source": [
        "#### (f) Create TRAINED_NET and train it on the full training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg1rXakcDAyA"
      },
      "source": [
        "%%time\n",
        "%matplotlib notebook\n",
        "\n",
        "# This training will be very slow on a CPU (>1hour per epoch). Ideally, this should be run in GPU mode (USE_GPU=True) \n",
        "# taking only a few minutes per epoch (depending on your GPU and batch size). Thus, before proceeding with this excercise,\n",
        "# it is highly advisable that you first finish debugging your net code. In particular, make sure that OVERFIT_NET behaves \n",
        "# reasonably, e.g. its loss monotonically decreases during training and its output is OK (for the image it was trained on). \n",
        "# Below we create another (deep) copy of untrained_net. Unlike OVERFIT_NET it will be trained on a full training dataset.\n",
        "trained_net = copy.deepcopy(untrained_net)\n",
        "\n",
        "# set loss function for the net\n",
        "trained_net.criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "\n",
        "# You can change the number of EPOCHS below. Since each epoch for TRAINED_NET iterates over all training dataset images,\n",
        "# the number of required epochs could be smaller compared to OFERFIT_NET where each epoch iterates over one-image-dataset)\n",
        "EPOCH = 2\n",
        "\n",
        "# switch to train mode (original untrained_net was set to eval mode)\n",
        "trained_net.train()\n",
        "\n",
        "optimizer = get_optimizer(trained_net)\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "\n",
        "loss_graph = []\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "plt.subplots_adjust(bottom=0.2,right=0.85,top=0.95)\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "\n",
        "for e in range(EPOCH):\n",
        "    loss = train(train_loader, trained_net, optimizer, loss_graph)\n",
        "    ax.clear()\n",
        "    ax.set_xlabel('iterations')\n",
        "    ax.set_ylabel('loss value')\n",
        "    ax.set_title('Training loss curve for TRAINED_NET')\n",
        "    ax.plot(loss_graph, label='training loss')\n",
        "    ax.legend(loc='upper right')\n",
        "    fig.canvas.draw()    \n",
        "    print(\"Epoch: {} Loss: {}\".format(e, loss))\n",
        "    \n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1WlCPA_DAyA"
      },
      "source": [
        "#### (g) Qualitative and quantitative evaluation of predictions (OVERFIT_NET vs TRAINED_NET):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmzLLUUZDAyA"
      },
      "source": [
        "# switch back to evaluation mode\n",
        "trained_net.eval()\n",
        "\n",
        "sample_img, sample_target = JointNormalize(*norm)(*JointToTensor()(*sample1))\n",
        "if USE_GPU:\n",
        "    sample_img = sample_img.cuda()\n",
        "sample_output_O = overfit_net.forward(sample_img[None])\n",
        "sample_output_T = trained_net.forward(sample_img[None])\n",
        "\n",
        "# computing mIOU (quantitative measure of accuracy for network predictions)\n",
        "pred_T = torch.argmax(sample_output_T, dim=1).cpu().numpy()[0]\n",
        "pred_O = torch.argmax(sample_output_O, dim=1).cpu().numpy()[0]\n",
        "gts = torch.from_numpy(np.array(sample1[1].convert('P'), dtype=np.int32)).long().numpy()\n",
        "gts[gts == 255] = -1\n",
        "conf_T = eval_semantic_segmentation(pred_T[None], gts[None])\n",
        "conf_O = eval_semantic_segmentation(pred_O[None], gts[None])\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(14,10))\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "plt.title('image sample')\n",
        "ax1.imshow(sample1[0])\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "plt.title('ground truth (target)')\n",
        "ax2.imshow(sample1[1])\n",
        "ax3 = fig.add_subplot(2,2,3)\n",
        "plt.title('OVERFIT_NET prediction (for its training image)')\n",
        "ax3.text(10, 25, 'mIoU = {:_>8.6f}'.format(conf_O['miou']), fontsize=20, color='white')\n",
        "ax3.imshow(colorize_mask(torch.argmax(sample_output_O, dim=1).cpu().numpy()[0]))\n",
        "ax4 = fig.add_subplot(2,2,4)\n",
        "plt.title('TRAINED_NET prediction (for one of its training images)')\n",
        "ax4.text(10, 25, 'mIoU = {:_>8.6f}'.format(conf_T['miou']), fontsize=20, color='white')\n",
        "ax4.imshow(colorize_mask(torch.argmax(sample_output_T, dim=1).cpu().numpy()[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAn-faIoDAyA"
      },
      "source": [
        "sample_img, sample_target = JointNormalize(*norm)(*JointToTensor()(*sample2))\n",
        "if USE_GPU:\n",
        "    sample_img = sample_img.cuda()\n",
        "sample_output_O = overfit_net.forward(sample_img[None])\n",
        "sample_output_T = trained_net.forward(sample_img[None])\n",
        "\n",
        "# computing mIOU (quantitative measure of accuracy for network predictions)\n",
        "pred_O = torch.argmax(sample_output_O, dim=1).cpu().numpy()[0]\n",
        "pred_T = torch.argmax(sample_output_T, dim=1).cpu().numpy()[0]\n",
        "gts = torch.from_numpy(np.array(sample2[1].convert('P'), dtype=np.int32)).long().numpy()\n",
        "gts[gts == 255] = -1\n",
        "conf_O = eval_semantic_segmentation(pred_O[None], gts[None])\n",
        "conf_T = eval_semantic_segmentation(pred_T[None], gts[None])\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(14,10))\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "plt.title('image sample')\n",
        "ax1.imshow(sample2[0])\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "plt.title('ground truth (target)')\n",
        "ax2.imshow(sample2[1])\n",
        "ax3 = fig.add_subplot(2,2,3)\n",
        "plt.title('OVERFIT_NET prediction (for image it has not seen)')\n",
        "ax3.text(10, 25, 'mIoU = {:_>8.6f}'.format(conf_O['miou']), fontsize=20, color='white')\n",
        "ax3.imshow(colorize_mask(torch.argmax(sample_output_O, dim=1).cpu().numpy()[0]))\n",
        "ax4 = fig.add_subplot(2,2,4)\n",
        "plt.title('TRAINED_NET prediction (for image it has not seen)')\n",
        "ax4.text(10, 25, 'mIoU = {:_>8.6f}'.format(conf_T['miou']), fontsize=20, color='white')\n",
        "ax4.imshow(colorize_mask(torch.argmax(sample_output_T, dim=1).cpu().numpy()[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBBEK629DAyA"
      },
      "source": [
        "#### (h) Evaluate TRAINED_NET on validation dataset.\n",
        "##### Run the validation loop for TRAINED_NET against the validation dataset (it has not seen): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWoyEvvtDAyA"
      },
      "source": [
        "%%time\n",
        "# This will be slow on CPU (around 1 hour). On GPU it should take only a few minutes (depending on your GPU).\n",
        "print(\"mIoU for TRAINED_NET over the validation dataset:{}\".format(validate(val_loader, trained_net)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOmQCL5NDAyA"
      },
      "source": [
        "# Problem 6\n",
        "\n",
        "#### For the network that you implemented, write a paragraph or two about limitations / bottlenecks about the work. What could be improved? What seems to be some obvious issues with the existing works?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YzMRbbuDAyA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}